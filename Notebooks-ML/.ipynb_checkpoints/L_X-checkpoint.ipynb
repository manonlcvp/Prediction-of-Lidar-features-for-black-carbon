{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94d8f33",
   "metadata": {},
   "source": [
    "# L_X : prédiction des variables physiques target avec les variables Lidar, fraction of coating et fractal dimension en input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d84f5a",
   "metadata": {},
   "source": [
    "#### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cdb53712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error, max_error, mean_absolute_percentage_error\n",
    "\n",
    "import math\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f797e4b",
   "metadata": {},
   "source": [
    "#### Données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "17aa9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./df_cleaned.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e8619070-f884-4b53-a9db-0f932092ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changer les noms de scolonnes avec / \n",
    "\n",
    "# Renommer la colonne existante en 'density_bc (g/cm^3)'\n",
    "df = df.rename(columns={'density_bc (g/cm^3)':'density_bc (g cm^3)'})\n",
    "df = df.rename(columns={'density_organics (g/cm^3)':'density_organics (g cm^3)'})\n",
    "df = df.rename(columns={'mr_total/bc':'mr_total bc'})\n",
    "df = df.rename(columns={'mr_nonBC/BC':'mr_nonBC BC'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "530cd41d-25af-4d58-b942-e2ff743b661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 2:23]  # données particules\n",
    "Y = df.iloc[:,23:31]  # données optiques\n",
    "L = df.iloc[:, [0, 1] + list(range(31, df.shape[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2f97073e-5ccf-451f-abb7-8db5ef588075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des variables intéressantes \n",
    "variables = ['primary_particle_size (nm)', \n",
    "             'vol_equi_radius_outer (nm)', \n",
    "             'vol_equi_radius_inner (nm)', \n",
    "             'equi_mobility_dia (nm)',\n",
    "                \"mass_bc (g)\"]\n",
    "\n",
    "X = X[variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5abe8-7df0-4167-b640-25d5157421b7",
   "metadata": {},
   "source": [
    "**Normalisation des inputs :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd914de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9fdedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            L, X,\n",
    "            test_size=0.30,\n",
    "            random_state=10)\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "X_test=X_test.reset_index(drop=True)\n",
    "Y_test=Y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_train_transformed = scaler.fit_transform(X_train)  #pt.fit_transform(X_train)\n",
    "X_test_transformed =  scaler.fit_transform(X_test)   #pt.transform(X_test)\n",
    "\n",
    "Y_train_transformed = pd.DataFrame(Y_train, columns=Y_train.columns)\n",
    "Y_test_transformed = pd.DataFrame(Y_test, columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39e26783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fractal_dimension', 'fraction_of_coating (%)', 'CR', 'BAE'], dtype='object')\n",
      "Index(['primary_particle_size (nm)', 'vol_equi_radius_outer (nm)',\n",
      "       'vol_equi_radius_inner (nm)', 'equi_mobility_dia (nm)', 'mass_bc (g)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_test.columns)\n",
    "print(Y_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d74a36",
   "metadata": {},
   "source": [
    "**Nombre de lignes à prendre en compte :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e899a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3883"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195cd25-f7ec-4c25-bfbd-22b9b1640114",
   "metadata": {},
   "source": [
    "## Régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e5041-0114-46fb-8785-d8edb153170d",
   "metadata": {},
   "source": [
    "### Définition\n",
    "La régression linéaire est une méthode statistique et machine learning utilisée pour modéliser la relation entre une **variable cible** (\\(Y\\)) et une ou plusieurs **variables explicatives** (\\(X\\)) à l'aide d'une fonction linéaire.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "Le modèle de régression linéaire est défini par :\n",
    "\n",
    "$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon $\n",
    "- $Y$ : Variable cible (dépendante).\n",
    "- $X_1, X_2, \\dots, X_p$ : Variables explicatives (indépendantes).\n",
    "- $\\beta_0$ : Intercept (ordonnée à l'origine).\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_p$ : Coefficients des variables.\n",
    "- $\\epsilon$ : Terme d'erreur (résiduel).\n",
    "\n",
    "---\n",
    "\n",
    "### Objectif\n",
    "- Trouver les coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ qui minimisent l'erreur entre les prédictions du modèle $\\hat{Y}$ et les valeurs réelles $Y$.\n",
    "- Cette minimisation est généralement réalisée par la **méthode des moindres carrés**, qui minimise la somme des carrés des résidus :\n",
    "$ \\text{Erreur} = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3aa3d9e0-83ab-4fb2-901b-956c39ef08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'primary_particle_size (nm)' enregistré sous Best_models/L_X/Linear\\primary_particle_size (nm)_best_model_linear.joblib\n",
      "Target Column 'primary_particle_size (nm)'\n",
      "MSE: 313.38643095061246, MAPE: 1.0078366851723815, R²: -34.692949417502895\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_outer (nm)' enregistré sous Best_models/L_X/Linear\\vol_equi_radius_outer (nm)_best_model_linear.joblib\n",
      "Target Column 'vol_equi_radius_outer (nm)'\n",
      "MSE: 8829.901226966607, MAPE: 1.0039967651168065, R²: -3.232781462860352\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_inner (nm)' enregistré sous Best_models/L_X/Linear\\vol_equi_radius_inner (nm)_best_model_linear.joblib\n",
      "Target Column 'vol_equi_radius_inner (nm)'\n",
      "MSE: 6194.120449184758, MAPE: 1.0035041966439493, R²: -3.7221632866438714\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'equi_mobility_dia (nm)' enregistré sous Best_models/L_X/Linear\\equi_mobility_dia (nm)_best_model_linear.joblib\n",
      "Target Column 'equi_mobility_dia (nm)'\n",
      "MSE: 172914.95226356955, MAPE: 1.0021909239533446, R²: -1.639923985228096\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'mass_bc (g)' enregistré sous Best_models/L_X/Linear\\mass_bc (g)_best_model_linear.joblib\n",
      "Target Column 'mass_bc (g)'\n",
      "MSE: 0.13032853173762785, MAPE: 508778398627317.44, R²: -4.7641104614244784e+27\n",
      "----------------------------------------\n",
      "                Target Column            MSE          MAPE            R2\n",
      "0  primary_particle_size (nm)     313.386431  1.007837e+00 -3.469295e+01\n",
      "1  vol_equi_radius_outer (nm)    8829.901227  1.003997e+00 -3.232781e+00\n",
      "2  vol_equi_radius_inner (nm)    6194.120449  1.003504e+00 -3.722163e+00\n",
      "3      equi_mobility_dia (nm)  172914.952264  1.002191e+00 -1.639924e+00\n",
      "4                 mass_bc (g)       0.130329  5.087784e+14 -4.764110e+27\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Initialiser le modèle de régression linéaire\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les métriques et les prédictions\n",
    "results_linear = []\n",
    "Y_pred_linear = pd.DataFrame()\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/L_X/Linear'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Ajuster le modèle sur la colonne actuelle (transformée)\n",
    "    linear_model.fit(X_train_transformed[:k, :], Y_train_transformed.iloc[:k, i])\n",
    "    \n",
    "    # Prédire les valeurs sur le jeu de test (transformé)\n",
    "    y_pred = linear_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "    \n",
    "    # Pas de transformation inverse ici : on utilise directement y_pred (transformation appliquée)\n",
    "    y_pred_original = y_pred  # Les prédictions sont déjà dans l'échelle transformée\n",
    "\n",
    "    # Calculer les métriques\n",
    "    mse = mean_squared_error(y_true, y_pred_original)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred_original)\n",
    "    r2 = r2_score(y_true, y_pred_original)\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results_linear.append({\n",
    "        'Target Column': col,\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "    \n",
    "    # Stocker les prédictions dans un DataFrame\n",
    "    Y_pred_linear[col] = y_pred_original\n",
    "\n",
    "    # Définir le chemin d'enregistrement du modèle pour chaque colonne\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_linear.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(linear_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les métriques pour la variable cible\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_linear = pd.DataFrame(results_linear)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_linear)\n",
    "\n",
    "# Afficher les prédictions (si nécessaire)\n",
    "# print(Y_pred_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce25db",
   "metadata": {},
   "source": [
    "## Random split (KRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a582e-1309-45d2-bf96-7aea4262676f",
   "metadata": {},
   "source": [
    "### Définition\n",
    "La **régression à noyau** (KRR) combine deux concepts puissants :\n",
    "1. **Régression ridge** : Une variante de la régression linéaire qui ajoute une pénalité pour limiter la complexité du modèle.\n",
    "2. **Trick du noyau** : Une technique permettant de modéliser des relations non linéaires en projetant les données dans un espace de caractéristiques de dimension supérieure.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "La KRR minimise la fonction coût suivante :\n",
    "\n",
    "$ \\text{Erreur} = ||Y - K \\alpha||^2 + \\lambda ||\\alpha||^2 $\n",
    "- $Y$ : Cibles réelles.\n",
    "- $K$ : Matrice de noyau, définie par $K_{ij} = k(X_i, X_j)$, où $k(\\cdot, \\cdot)$ est une fonction de noyau.\n",
    "- $\\alpha$ : Coefficients à déterminer.\n",
    "- $\\lambda$ : Hyperparamètre de régularisation qui contrôle le compromis biais/variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Fonction de noyau\n",
    "La fonction de noyau $k(X_i, X_j)$ mesure la similarité entre deux observations. Les noyaux courants sont :\n",
    "- **Linéaire** : $k(X_i, X_j) = X_i^T X_j$\n",
    "- **RBF (Radial Basis Function)** : $k(X_i, X_j) = \\exp\\left(-\\frac{||X_i - X_j||^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "Le choix du noyau permet de capturer des relations linéaires ou non linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d04afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'primary_particle_size (nm)' enregistré sous Best_models/L_X/KRR\\primary_particle_size (nm)_best_model_KRR.joblib\n",
      "Target Column 'primary_particle_size (nm)'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "MSE: 0.08299067469096243, MAPE: 0.008692784478014276, R²: 0.9905478295761446\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_outer (nm)' enregistré sous Best_models/L_X/KRR\\vol_equi_radius_outer (nm)_best_model_KRR.joblib\n",
      "Target Column 'vol_equi_radius_outer (nm)'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MSE: 413.3442335004959, MAPE: 0.19072850479377063, R²: 0.8018555627782294\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_inner (nm)' enregistré sous Best_models/L_X/KRR\\vol_equi_radius_inner (nm)_best_model_KRR.joblib\n",
      "Target Column 'vol_equi_radius_inner (nm)'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MSE: 229.64542237960305, MAPE: 0.1934366440004059, R²: 0.8249270107994892\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'equi_mobility_dia (nm)' enregistré sous Best_models/L_X/KRR\\equi_mobility_dia (nm)_best_model_KRR.joblib\n",
      "Target Column 'equi_mobility_dia (nm)'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MSE: 13287.122054356709, MAPE: 0.3165353221477301, R²: 0.7971430940657892\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'mass_bc (g)' enregistré sous Best_models/L_X/KRR\\mass_bc (g)_best_model_KRR.joblib\n",
      "Target Column 'mass_bc (g)'\n",
      "Best Parameters: {'alpha': 1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MSE: 6.855296209819496e-30, MAPE: 0.4755596432507138, R²: 0.7494072253103166\n",
      "----------------------------------------\n",
      "                Target Column  alpha kernel  gamma           MSE      MAPE  \\\n",
      "0  primary_particle_size (nm)    0.1    rbf    0.1  8.299067e-02  0.008693   \n",
      "1  vol_equi_radius_outer (nm)    0.1    rbf   10.0  4.133442e+02  0.190729   \n",
      "2  vol_equi_radius_inner (nm)    0.1    rbf   10.0  2.296454e+02  0.193437   \n",
      "3      equi_mobility_dia (nm)    0.1    rbf   10.0  1.328712e+04  0.316535   \n",
      "4                 mass_bc (g)    1.0    rbf   10.0  6.855296e-30  0.475560   \n",
      "\n",
      "         R2  \n",
      "0  0.990548  \n",
      "1  0.801856  \n",
      "2  0.824927  \n",
      "3  0.797143  \n",
      "4  0.749407  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour Kernel Ridge Regression\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10],           # Paramètre de régularisation\n",
    "    'kernel': ['linear', 'rbf'],     # Choix de noyaux\n",
    "    'gamma': [0.1, 1, 10]            # Paramètre pour le noyau 'rbf'\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/L_X/KRR'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle Kernel Ridge\n",
    "    krr = KernelRidge()\n",
    "\n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=krr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',  # Utiliser MAPE pour optimiser\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "\n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "\n",
    "    # Prédire sur les données de test\n",
    "    y_pred = best_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "\n",
    "    # Calculer les erreurs\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Stocker les résultats\n",
    "    best_params_list.append({\n",
    "        'Target Column': col,\n",
    "        'alpha': grid_search.best_params_['alpha'],\n",
    "        'kernel': grid_search.best_params_['kernel'],\n",
    "        'gamma': grid_search.best_params_['gamma'],\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_KRR.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les résultats pour chaque colonne\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_KRR = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_KRR)\n",
    "\n",
    "# Créer un DataFrame pour les prédictions\n",
    "Y_pred_KRR = pd.DataFrame()\n",
    "for column, model in best_models.items():\n",
    "    Y_pred_KRR[column] = model.predict(X_test_transformed[:k, :])\n",
    "\n",
    "# Afficher les prédictions\n",
    "#print(Y_pred_KRR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1e526",
   "metadata": {},
   "source": [
    "## Gradient boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ee1e7-a501-4e82-90cd-f04c0b7afe47",
   "metadata": {},
   "source": [
    "### Définition\n",
    "Le **Gradient Boosting** est une technique d'ensemble qui construit un modèle puissant en combinant plusieurs modèles faibles (souvent des arbres de décision) de manière séquentielle. À chaque étape, le modèle suivant corrige les erreurs du modèle précédent en optimisant une fonction de perte grâce à la descente de gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "L'objectif est de minimiser une fonction de perte $L(Y, \\hat{Y})$, où :\n",
    "- $Y$ : Cibles réelles.\n",
    "- $\\hat{Y}$ : Prédictions du modèle.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparamètres clés\n",
    "- **Nombre d'estimateurs** $(n\\_estimators)$ : Nombre total d'arbres.\n",
    "- **Taux d'apprentissage** $(learning\\_rate)$ : Contrôle la contribution de chaque arbre.\n",
    "- **Profondeur maximale** $(max\\_depth)$ : Limite la complexité des arbres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d02f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'primary_particle_size (nm)' enregistré sous Best_models/L_X/GB\\primary_particle_size (nm)_best_model_GBR.joblib\n",
      "Variable de sortie 'primary_particle_size (nm)'\n",
      "Meilleurs paramètres : {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -5.099028563884038e-10\n",
      "MSE: 1.8808758852149646e-16, MAPE: 4.981081117982237e-10, R²: 1.0\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_outer (nm)' enregistré sous Best_models/L_X/GB\\vol_equi_radius_outer (nm)_best_model_GBR.joblib\n",
      "Variable de sortie 'vol_equi_radius_outer (nm)'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 300}\n",
      "Meilleur score (MAPE négatif) : -0.18253469534857966\n",
      "MSE: 561.9339778943081, MAPE: 0.23527767937776786, R²: 0.7306262364840153\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_inner (nm)' enregistré sous Best_models/L_X/GB\\vol_equi_radius_inner (nm)_best_model_GBR.joblib\n",
      "Variable de sortie 'vol_equi_radius_inner (nm)'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 300}\n",
      "Meilleur score (MAPE négatif) : -0.17356944997722012\n",
      "MSE: 291.7951006828012, MAPE: 0.2304846484983126, R²: 0.7775464453797909\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'equi_mobility_dia (nm)' enregistré sous Best_models/L_X/GB\\equi_mobility_dia (nm)_best_model_GBR.joblib\n",
      "Variable de sortie 'equi_mobility_dia (nm)'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 300}\n",
      "Meilleur score (MAPE négatif) : -0.36164054896300474\n",
      "MSE: 20464.584955673818, MAPE: 0.46628792708165573, R²: 0.6875634642059607\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'mass_bc (g)' enregistré sous Best_models/L_X/GB\\mass_bc (g)_best_model_GBR.joblib\n",
      "Variable de sortie 'mass_bc (g)'\n",
      "Meilleurs paramètres : {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 10}\n",
      "Meilleur score (MAPE négatif) : -5.458396267515861\n",
      "MSE: 2.7356718295439056e-29, MAPE: 5.199663342616369, R²: -1.4548494412514046e-05\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour GradientBoostingRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 300, 500],\n",
    "    'learning_rate': [0.05, 0.1, 0.5],\n",
    "    'max_depth': [2, 3]\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# DataFrame pour stocker les prédictions pour chaque variable cible\n",
    "Y_pred_GB = pd.DataFrame()\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/L_X/GB'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable de sortie (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle de GradientBoostingRegressor\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    \n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=gbr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "    \n",
    "    # Enregistrer le meilleur modèle pour la variable cible actuelle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "    \n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_pred = best_model.predict(X_test_transformed)\n",
    "    y_true = Y_test.iloc[:, i]\n",
    "    \n",
    "    # Ajouter les prédictions au DataFrame Y_pred_GB\n",
    "    Y_pred_GB[col] = y_pred\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Ajouter les meilleurs paramètres et les scores dans la liste des meilleurs paramètres\n",
    "    best_params_list.append({\n",
    "        'Variable': col,\n",
    "        'n_estimators': grid_search.best_params_['n_estimators'],\n",
    "        'learning_rate': grid_search.best_params_['learning_rate'],\n",
    "        'max_depth': grid_search.best_params_['max_depth'],\n",
    "        'Best Score (MAPE Negatif)': grid_search.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_GBR.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les meilleurs hyperparamètres et les scores pour la variable cible actuelle\n",
    "    print(f\"Variable de sortie '{col}'\")\n",
    "    print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "    print(\"Meilleur score (MAPE négatif) :\", grid_search.best_score_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour stocker les meilleurs paramètres et les métriques de chaque modèle\n",
    "params_GB = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le DataFrame des meilleurs paramètres\n",
    "#print(params_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f5c8b",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f865ab4-e6c5-4540-997d-97604dc111ce",
   "metadata": {},
   "source": [
    "### Définition\n",
    "XGBoost est une implémentation avancée et optimisée de la méthode Gradient Boosting. Elle est conçue pour être :\n",
    "- **Rapide** grâce à des optimisations matérielles et algorithmiques.\n",
    "- **Précise** avec des techniques intégrées de régularisation.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "\n",
    "L'objectif est de minimiser une fonction de perte régulière définie par : \n",
    "\n",
    "$ \\mathcal{L}(\\Theta) = \\sum_{i=1}^{n} L(Y_i, \\hat{Y}_i) + \\sum_{k=1}^{K} \\Omega(f_k) $\n",
    "- $L(Y_i, \\hat{Y}_i)$ : Fonction de perte\n",
    "- $\\Omega(f_k)$ : Terme de régularisation pour éviter le surapprentissage.\n",
    "\n",
    "$ \\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2 $\n",
    "  - $T$ : Nombre de feuilles dans l'arbre.\n",
    "  - $w$ : Poids des feuilles.\n",
    "  - $\\gamma, \\lambda$ : Hyperparamètres de régularisation.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparamètres clés\n",
    "- **Nombre d'estimateurs** $(n\\_estimators)$ : Nombre total d'arbres.\n",
    "- **Taux d'apprentissage** $(learning\\_rate)$ : Contrôle la contribution de chaque arbre.\n",
    "- **Profondeur maximale** $(max\\_depth)$ : Limite la complexité des arbres.\n",
    "- **Subsample** et **ColSampleByTree** : Contrôle du sous-échantillonnage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c210111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'primary_particle_size (nm)' enregistré sous Best_models/L_X/XGB\\primary_particle_size (nm)_best_model_XGB.joblib\n",
      "Target Column 'primary_particle_size (nm)'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1}\n",
      "MSE: 7.856459090397048e-10, MAPE: 1.0401422174487148e-06, R²: 0.9999999999105194\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_outer (nm)' enregistré sous Best_models/L_X/XGB\\vol_equi_radius_outer (nm)_best_model_XGB.joblib\n",
      "Target Column 'vol_equi_radius_outer (nm)'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 341.4266021039619, MAPE: 0.17821185516499957, R²: 0.8363306502342925\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_inner (nm)' enregistré sous Best_models/L_X/XGB\\vol_equi_radius_inner (nm)_best_model_XGB.joblib\n",
      "Target Column 'vol_equi_radius_inner (nm)'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 205.74788544248864, MAPE: 0.18470670766124891, R²: 0.8431455896100627\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'equi_mobility_dia (nm)' enregistré sous Best_models/L_X/XGB\\equi_mobility_dia (nm)_best_model_XGB.joblib\n",
      "Target Column 'equi_mobility_dia (nm)'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 13124.618391341308, MAPE: 0.3099647780878132, R²: 0.7996240670069138\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'mass_bc (g)' enregistré sous Best_models/L_X/XGB\\mass_bc (g)_best_model_XGB.joblib\n",
      "Target Column 'mass_bc (g)'\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300, 'subsample': 0.8}\n",
      "MSE: 2.735718002125476e-29, MAPE: 5.186792607318488, R²: -3.142670795175384e-05\n",
      "----------------------------------------\n",
      "                Target Column  n_estimators  learning_rate  max_depth  \\\n",
      "0  primary_particle_size (nm)           100           0.30          7   \n",
      "1  vol_equi_radius_outer (nm)           500           0.05          7   \n",
      "2  vol_equi_radius_inner (nm)           500           0.05          7   \n",
      "3      equi_mobility_dia (nm)           500           0.05          7   \n",
      "4                 mass_bc (g)           300           0.10          3   \n",
      "\n",
      "   subsample  colsample_bytree           MSE      MAPE        R2  \n",
      "0        1.0               1.0  7.856459e-10  0.000001  1.000000  \n",
      "1        0.8               1.0  3.414266e+02  0.178212  0.836331  \n",
      "2        0.8               1.0  2.057479e+02  0.184707  0.843146  \n",
      "3        0.8               1.0  1.312462e+04  0.309965  0.799624  \n",
      "4        0.8               0.8  2.735718e-29  5.186793 -0.000031  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Nombre d'arbres\n",
    "    'learning_rate': [0.05, 0.1, 0.3],  # Taux d'apprentissage\n",
    "    'max_depth': [3, 5, 7],  # Profondeur maximale des arbres\n",
    "    'subsample': [0.8, 1],  # Fraction des échantillons utilisés pour entraîner chaque arbre\n",
    "    'colsample_bytree': [0.8, 1]  # Fraction des caractéristiques utilisées pour chaque arbre\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/L_X/XGB'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle XGBoost\n",
    "    xgbr = XGBRegressor(objective='reg:squarederror', n_jobs=-1)  # Configuré pour minimiser l'erreur quadratique\n",
    "\n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgbr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',  # Optimisation avec MAPE\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "\n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "\n",
    "    # Prédire sur les données de test\n",
    "    y_pred = best_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "\n",
    "    # Calculer les erreurs\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Stocker les résultats\n",
    "    best_params_list.append({\n",
    "        'Target Column': col,\n",
    "        'n_estimators': grid_search.best_params_['n_estimators'],\n",
    "        'learning_rate': grid_search.best_params_['learning_rate'],\n",
    "        'max_depth': grid_search.best_params_['max_depth'],\n",
    "        'subsample': grid_search.best_params_['subsample'],\n",
    "        'colsample_bytree': grid_search.best_params_['colsample_bytree'],\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_XGB.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les résultats pour chaque colonne\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_XGB = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_XGB)\n",
    "\n",
    "# Créer un DataFrame pour les prédictions\n",
    "Y_pred_XGB = pd.DataFrame()\n",
    "for column, model in best_models.items():\n",
    "    Y_pred_XGB[column] = model.predict(X_test_transformed[:k, :])\n",
    "\n",
    "# Afficher les prédictions (si nécessaire)\n",
    "# print(Y_pred_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f3dcb",
   "metadata": {},
   "source": [
    "## ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e671b-5270-4256-a272-3e35aa4e7f67",
   "metadata": {},
   "source": [
    "### Définition\n",
    "Les **réseaux de neurones artificiels** (ANN) sont des modèles inspirés du cerveau humain, capables de détecter des motifs complexes dans les données. Ils sont constitués de couches de **neurones** interconnectés, organisées en trois types principaux de couches :\n",
    "- **Entrée** : Reçoit les données d'entrée.\n",
    "- **Cachées** : Effectuent les transformations et calculs complexes.\n",
    "- **Sortie** : Produit les prédictions finales.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "\n",
    "Chaque neurone dans une couche effectue un calcul basé sur une somme pondérée des entrées, suivie d'une activation non linéaire :\n",
    "\n",
    "$ z = \\sum_{i=1}^{n} w_i x_i + b $\n",
    "\n",
    "$a = \\phi(z)$\n",
    "- $w_i$ : Poids associés aux entrées.\n",
    "- $x_i$ : Entrées.\n",
    "- $b$ : Biais.\n",
    "- $\\phi$ : Fonction d'activation (ex. ReLU, sigmoïde, tanh).\n",
    "\n",
    "Les poids et les biais sont ajustés pendant l'entraînement pour minimiser une fonction de perte.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparamètres clés\n",
    "\n",
    "- **hidden_layer_sizes** : Détermine la structure des couches cachées. Chaque tuple dans cette liste représente le nombre de neurones dans chaque couche cachée.\n",
    "- **activation** : Fonction d'activation à utiliser dans les couches cachées.\n",
    "- **learning_rate_init** : Taux d'apprentissage initial, contrôlant la vitesse à laquelle le modèle ajuste ses poids.\n",
    "- **max_iter** : Nombre maximal d'itérations (ou d'époques) pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "974cf1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'primary_particle_size (nm)' enregistré sous Best_models/L_X/ANN\\primary_particle_size (nm)_best_model_ANN.joblib\n",
      "Target Column 'primary_particle_size (nm)'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01, 'max_iter': 500}\n",
      "MSE: 4.112291894448539, MAPE: 0.08158167735192279, R²: 0.531633114639573\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_outer (nm)' enregistré sous Best_models/L_X/ANN\\vol_equi_radius_outer (nm)_best_model_ANN.joblib\n",
      "Target Column 'vol_equi_radius_outer (nm)'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.001, 'max_iter': 500}\n",
      "MSE: 1685.9370305949412, MAPE: 0.5868795756328818, R²: 0.19181394817214292\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'vol_equi_radius_inner (nm)' enregistré sous Best_models/L_X/ANN\\vol_equi_radius_inner (nm)_best_model_ANN.joblib\n",
      "Target Column 'vol_equi_radius_inner (nm)'\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.001, 'max_iter': 500}\n",
      "MSE: 1029.1143679394995, MAPE: 0.5365071782466495, R²: 0.2154421074131322\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'equi_mobility_dia (nm)' enregistré sous Best_models/L_X/ANN\\equi_mobility_dia (nm)_best_model_ANN.joblib\n",
      "Target Column 'equi_mobility_dia (nm)'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 500}\n",
      "MSE: 69600.903467471, MAPE: 1.2777369695129381, R²: -0.06260963584716772\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'mass_bc (g)' enregistré sous Best_models/L_X/ANN\\mass_bc (g)_best_model_ANN.joblib\n",
      "Target Column 'mass_bc (g)'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.01, 'max_iter': 500}\n",
      "MSE: 5.141014671346651e-07, MAPE: 867396661335.0334, R²: -1.8792785778793507e+22\n",
      "----------------------------------------\n",
      "                Target Column hidden_layer_sizes activation  \\\n",
      "0  primary_particle_size (nm)           (50, 50)       tanh   \n",
      "1  vol_equi_radius_outer (nm)         (100, 100)       tanh   \n",
      "2  vol_equi_radius_inner (nm)         (100, 100)       relu   \n",
      "3      equi_mobility_dia (nm)           (50, 50)       tanh   \n",
      "4                 mass_bc (g)         (100, 100)       tanh   \n",
      "\n",
      "   learning_rate_init  max_iter           MSE          MAPE            R2  \n",
      "0               0.010       500  4.112292e+00  8.158168e-02  5.316331e-01  \n",
      "1               0.001       500  1.685937e+03  5.868796e-01  1.918139e-01  \n",
      "2               0.001       500  1.029114e+03  5.365072e-01  2.154421e-01  \n",
      "3               0.001       500  6.960090e+04  1.277737e+00 -6.260964e-02  \n",
      "4               0.010       500  5.141015e-07  8.673967e+11 -1.879279e+22  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour MLPRegressor (ANN)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Architectures des couches cachées\n",
    "    'activation': ['relu', 'tanh'],                               # Fonctions d'activation\n",
    "    'learning_rate_init': [0.001, 0.01],                          # Taux d'apprentissage initial\n",
    "    'max_iter': [500, 1000]                                       # Nombre maximal d'itérations\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/L_X/ANN'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle ANN (MLPRegressor)\n",
    "    mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=mlp,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',  # Optimisation avec MAPE\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "\n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "\n",
    "    # Prédire sur les données de test\n",
    "    y_pred = best_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "\n",
    "    # Calculer les erreurs\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Stocker les résultats\n",
    "    best_params_list.append({\n",
    "        'Target Column': col,\n",
    "        'hidden_layer_sizes': grid_search.best_params_['hidden_layer_sizes'],\n",
    "        'activation': grid_search.best_params_['activation'],\n",
    "        'learning_rate_init': grid_search.best_params_['learning_rate_init'],\n",
    "        'max_iter': grid_search.best_params_['max_iter'],\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_ANN.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les résultats pour chaque colonne\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_ANN = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_ANN)\n",
    "\n",
    "# Créer un DataFrame pour les prédictions\n",
    "Y_pred_ANN = pd.DataFrame()\n",
    "for column, model in best_models.items():\n",
    "    Y_pred_ANN[column] = model.predict(X_test_transformed[:k, :])\n",
    "\n",
    "# Afficher les prédictions (si nécessaire)\n",
    "# print(Y_pred_ANN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94d8f33",
   "metadata": {},
   "source": [
    "# Prédiction des données optiques avec les données physiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8814a-3a7e-4bbc-8848-b59747ee62b9",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous comparons différents modèles afin de prédire au mieux les données optiques à partir des données physiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d84f5a",
   "metadata": {},
   "source": [
    "#### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb53712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_absolute_error, max_error, mean_absolute_percentage_error\n",
    "\n",
    "import math\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f797e4b",
   "metadata": {},
   "source": [
    "#### Données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17aa9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./df_cleaned.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002142b9-6017-4380-9cc4-263e1f5241c4",
   "metadata": {},
   "source": [
    "La première étape consiste à diviser notre jeu de données en trois parties (physiques, optiques et lidar) afin de pouvoir tester différentes prédictions dans différents sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a8870b-6796-4af6-b55c-1be1f09c436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :23]  # données particules\n",
    "Y = df.iloc[:,23:31]  # données optiques\n",
    "L = df.iloc[:,31:]  # données Lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd914de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ec16c-d495-41cd-b545-f82c5b2f2415",
   "metadata": {},
   "source": [
    "Nous divisons ensuite notre jeu de données de sorte à avoir 70% des données pour l'entraînement et 30% pour le test.\n",
    "L'ensemble de test est utilisé pour vérifier si le modèle est capable de généraliser sur des données qu'il n'a jamais vues.\n",
    "Ensuite, nous effectuons une normalisation/standardisation des caractéristiques afin d'améliorer la convergence des modèles.\n",
    "Enfin, nous préparons les données et les rendons compatibles avec des modèles machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fdedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y,\n",
    "            test_size=0.30,\n",
    "            random_state=10)\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "X_test=X_test.reset_index(drop=True)\n",
    "Y_test=Y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_train_transformed = scaler.fit_transform(X_train)  #pt.fit_transform(X_train)\n",
    "X_test_transformed =  scaler.fit_transform(X_test)   #pt.transform(X_test)\n",
    "\n",
    "Y_train_transformed = pd.DataFrame(pt.fit_transform(Y_train), columns=Y_train.columns)\n",
    "Y_test_transformed = pd.DataFrame(pt.transform(Y_test), columns=Y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e26783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fractal_dimension', 'fraction_of_coating (%)',\n",
      "       'primary_particle_size (nm)', 'number_of_primary_particles',\n",
      "       'vol_equi_radius_outer (nm)', 'vol_equi_radius_inner (nm)',\n",
      "       'equi_mobility_dia (nm)', 'mie_epsilon', 'length_scale_factor',\n",
      "       'm_real_bc', 'm_im_bc', 'm_real_organics', 'm_im_organics',\n",
      "       'volume_total (nm^3)', 'volume_bc (nm^3)', 'volume_organics (nm^3)',\n",
      "       'density_bc (g/cm^3)', 'density_organics (g/cm^3)', 'mass_bc (g)',\n",
      "       'mass_organics (g)', 'mass_total  (g)', 'mr_total/bc', 'mr_nonBC/BC'],\n",
      "      dtype='object')\n",
      "Index(['MEC_530', 'MEC_467', 'Cbac_530', 'Cbac_467', 'MBC_530', 'MBC_467',\n",
      "       'LR_530', 'LR_467'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "para  = X.columns\n",
    "print(para)\n",
    "print(Y_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d74a36",
   "metadata": {},
   "source": [
    "Nombre de lignes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e899a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3883"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f51ed9-eaaf-44a4-bbb8-221301cfb041",
   "metadata": {},
   "source": [
    "## Régression linéaire "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8fbd8-03e0-46ca-8766-809b4ae80036",
   "metadata": {},
   "source": [
    "### Définition\n",
    "La régression linéaire est une méthode statistique et machine learning utilisée pour modéliser la relation entre une **variable cible** (\\(Y\\)) et une ou plusieurs **variables explicatives** (\\(X\\)) à l'aide d'une fonction linéaire.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "Le modèle de régression linéaire est défini par :\n",
    "\n",
    "$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon $\n",
    "- $Y$ : Variable cible (dépendante).\n",
    "- $X_1, X_2, \\dots, X_p$ : Variables explicatives (indépendantes).\n",
    "- $\\beta_0$ : Intercept (ordonnée à l'origine).\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_p$ : Coefficients des variables.\n",
    "- $\\epsilon$ : Terme d'erreur (résiduel).\n",
    "\n",
    "---\n",
    "\n",
    "### Objectif\n",
    "- Trouver les coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$ qui minimisent l'erreur entre les prédictions du modèle $\\hat{Y}$ et les valeurs réelles $Y$.\n",
    "- Cette minimisation est généralement réalisée par la **méthode des moindres carrés**, qui minimise la somme des carrés des résidus :\n",
    "$ \\text{Erreur} = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d099366-3631-436b-b725-fd09fa78e11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MEC_530' enregistré sous Best_models/X_Y/Linear\\MEC_530_best_model_linear.joblib\n",
      "Target Column 'MEC_530'\n",
      "MSE: 3.136798373603482e+25, MAPE: 1.0000000042462898, R²: -16.632290709645687\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MEC_467' enregistré sous Best_models/X_Y/Linear\\MEC_467_best_model_linear.joblib\n",
      "Target Column 'MEC_467'\n",
      "MSE: 5.582083114001249e+25, MAPE: 0.9999999998685476, R²: -30.577152475562585\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_530' enregistré sous Best_models/X_Y/Linear\\Cbac_530_best_model_linear.joblib\n",
      "Target Column 'Cbac_530'\n",
      "MSE: 211638836647.7084, MAPE: 172589647310.21005, R²: -8.164406306485997e+16\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_467' enregistré sous Best_models/X_Y/Linear\\Cbac_467_best_model_linear.joblib\n",
      "Target Column 'Cbac_467'\n",
      "MSE: 269075608686.23218, MAPE: 116755745489.49971, R²: -4.465803129108465e+16\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_530' enregistré sous Best_models/X_Y/Linear\\MBC_530_best_model_linear.joblib\n",
      "Target Column 'MBC_530'\n",
      "MSE: 6.634268086442083e+21, MAPE: 0.9999915681873236, R²: -2.6991267978483235\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_467' enregistré sous Best_models/X_Y/Linear\\MBC_467_best_model_linear.joblib\n",
      "Target Column 'MBC_467'\n",
      "MSE: 1.529992900734317e+22, MAPE: 0.9999939082960339, R²: -3.0835839931174718\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_530' enregistré sous Best_models/X_Y/Linear\\LR_530_best_model_linear.joblib\n",
      "Target Column 'LR_530'\n",
      "MSE: 230398784503.72607, MAPE: 5018.521224903647, R²: -1907422.422771109\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_467' enregistré sous Best_models/X_Y/Linear\\LR_467_best_model_linear.joblib\n",
      "Target Column 'LR_467'\n",
      "MSE: 329364630424.2134, MAPE: 6383.180820726273, R²: -5072277.276048022\n",
      "----------------------------------------\n",
      "  Target Column           MSE          MAPE            R2\n",
      "0       MEC_530  3.136798e+25  1.000000e+00 -1.663229e+01\n",
      "1       MEC_467  5.582083e+25  1.000000e+00 -3.057715e+01\n",
      "2      Cbac_530  2.116388e+11  1.725896e+11 -8.164406e+16\n",
      "3      Cbac_467  2.690756e+11  1.167557e+11 -4.465803e+16\n",
      "4       MBC_530  6.634268e+21  9.999916e-01 -2.699127e+00\n",
      "5       MBC_467  1.529993e+22  9.999939e-01 -3.083584e+00\n",
      "6        LR_530  2.303988e+11  5.018521e+03 -1.907422e+06\n",
      "7        LR_467  3.293646e+11  6.383181e+03 -5.072277e+06\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Initialiser le modèle de régression linéaire\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les métriques et les prédictions\n",
    "results_linear = []\n",
    "Y_pred_linear = pd.DataFrame()\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/X_Y/Linear'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Ajuster le modèle sur la colonne actuelle (transformée)\n",
    "    linear_model.fit(X_train_transformed[:k, :], Y_train_transformed.iloc[:k, i])\n",
    "    \n",
    "    # Prédire les valeurs sur le jeu de test (transformé)\n",
    "    y_pred = linear_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "    \n",
    "    # Pas de transformation inverse ici : on utilise directement y_pred (transformation appliquée)\n",
    "    y_pred_original = y_pred  # Les prédictions sont déjà dans l'échelle transformée\n",
    "\n",
    "    # Calculer les métriques\n",
    "    mse = mean_squared_error(y_true, y_pred_original)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred_original)\n",
    "    r2 = r2_score(y_true, y_pred_original)\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results_linear.append({\n",
    "        'Target Column': col,\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "    \n",
    "    # Stocker les prédictions dans un DataFrame\n",
    "    Y_pred_linear[col] = y_pred_original\n",
    "\n",
    "    # Définir le chemin d'enregistrement du modèle pour chaque colonne\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_linear.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(linear_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les métriques pour la variable cible\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_linear = pd.DataFrame(results_linear)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_linear)\n",
    "\n",
    "# Afficher les prédictions (si nécessaire)\n",
    "# print(Y_pred_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce25db",
   "metadata": {},
   "source": [
    "## Random split (KRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db815884-e617-4882-ba49-8bd8cf6f6050",
   "metadata": {},
   "source": [
    "### Définition\n",
    "La **régression à noyau** (KRR) combine deux concepts puissants :\n",
    "1. **Régression ridge** : Une variante de la régression linéaire qui ajoute une pénalité pour limiter la complexité du modèle.\n",
    "2. **Trick du noyau** : Une technique permettant de modéliser des relations non linéaires en projetant les données dans un espace de caractéristiques de dimension supérieure.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "La KRR minimise la fonction coût suivante :\n",
    "\n",
    "$ \\text{Erreur} = ||Y - K \\alpha||^2 + \\lambda ||\\alpha||^2 $\n",
    "- $Y$ : Cibles réelles.\n",
    "- $K$ : Matrice de noyau, définie par $K_{ij} = k(X_i, X_j)$, où $k(\\cdot, \\cdot)$ est une fonction de noyau.\n",
    "- $\\alpha$ : Coefficients à déterminer.\n",
    "- $\\lambda$ : Hyperparamètre de régularisation qui contrôle le compromis biais/variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Fonction de noyau\n",
    "La fonction de noyau $k(X_i, X_j)$ mesure la similarité entre deux observations. Les noyaux courants sont :\n",
    "- **Linéaire** : $k(X_i, X_j) = X_i^T X_j$\n",
    "- **RBF (Radial Basis Function)** : $k(X_i, X_j) = \\exp\\left(-\\frac{||X_i - X_j||^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "Le choix du noyau permet de capturer des relations linéaires ou non linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d04afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MEC_530' enregistré sous Best_models/X_Y/KRR\\MEC_530_best_model_KRR.joblib\n",
      "Target Column 'MEC_530'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "MSE: 6.84615814219844e+22, MAPE: 0.03590289031284647, R²: 0.9615169876319536\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MEC_467' enregistré sous Best_models/X_Y/KRR\\MEC_467_best_model_KRR.joblib\n",
      "Target Column 'MEC_467'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "MSE: 3.856138538184798e+23, MAPE: 0.04995924485428618, R²: 0.7818630212048701\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_530' enregistré sous Best_models/X_Y/KRR\\Cbac_530_best_model_KRR.joblib\n",
      "Target Column 'Cbac_530'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MSE: 2.703903704684829e-06, MAPE: 0.24259293149220199, R²: -0.04308683643951161\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_467' enregistré sous Best_models/X_Y/KRR\\Cbac_467_best_model_KRR.joblib\n",
      "Target Column 'Cbac_467'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "MSE: 6.280824761177408e-06, MAPE: 0.32173201841754046, R²: -0.042418040371534405\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_530' enregistré sous Best_models/X_Y/KRR\\MBC_530_best_model_KRR.joblib\n",
      "Target Column 'MBC_530'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "MSE: 1.259901397683965e+20, MAPE: 0.06621489104552541, R²: 0.9297505774247551\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_467' enregistré sous Best_models/X_Y/KRR\\MBC_467_best_model_KRR.joblib\n",
      "Target Column 'MBC_467'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "MSE: 3.238673904239602e+20, MAPE: 0.08031947530236538, R²: 0.9135590962027823\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_530' enregistré sous Best_models/X_Y/KRR\\LR_530_best_model_KRR.joblib\n",
      "Target Column 'LR_530'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "MSE: 17849.345205708338, MAPE: 0.20617579904824881, R²: 0.8522289985173742\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_467' enregistré sous Best_models/X_Y/KRR\\LR_467_best_model_KRR.joblib\n",
      "Target Column 'LR_467'\n",
      "Best Parameters: {'alpha': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "MSE: 10492.321487734633, MAPE: 0.2069444704638458, R²: 0.838416243180082\n",
      "----------------------------------------\n",
      "  Target Column  alpha kernel  gamma           MSE      MAPE        R2\n",
      "0       MEC_530    0.1    rbf    0.1  6.846158e+22  0.035903  0.961517\n",
      "1       MEC_467    0.1    rbf    1.0  3.856139e+23  0.049959  0.781863\n",
      "2      Cbac_530    0.1    rbf   10.0  2.703904e-06  0.242593 -0.043087\n",
      "3      Cbac_467    0.1    rbf   10.0  6.280825e-06  0.321732 -0.042418\n",
      "4       MBC_530    0.1    rbf    1.0  1.259901e+20  0.066215  0.929751\n",
      "5       MBC_467    0.1    rbf    1.0  3.238674e+20  0.080319  0.913559\n",
      "6        LR_530    0.1    rbf    1.0  1.784935e+04  0.206176  0.852229\n",
      "7        LR_467    0.1    rbf    1.0  1.049232e+04  0.206944  0.838416\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour Kernel Ridge Regression\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10],           # Paramètre de régularisation\n",
    "    'kernel': ['linear', 'rbf'],     # Choix de noyaux\n",
    "    'gamma': [0.1, 1, 10]            # Paramètre pour le noyau 'rbf'\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/X_Y/KRR'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle Kernel Ridge\n",
    "    krr = KernelRidge()\n",
    "\n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=krr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',  # Utiliser MAPE pour optimiser\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "\n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "\n",
    "    # Prédire sur les données de test\n",
    "    y_pred = best_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "\n",
    "    # Calculer les erreurs\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Stocker les résultats\n",
    "    best_params_list.append({\n",
    "        'Target Column': col,\n",
    "        'alpha': grid_search.best_params_['alpha'],\n",
    "        'kernel': grid_search.best_params_['kernel'],\n",
    "        'gamma': grid_search.best_params_['gamma'],\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_KRR.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les résultats pour chaque colonne\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_KRR = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_KRR)\n",
    "\n",
    "# Créer un DataFrame pour les prédictions\n",
    "Y_pred_KRR = pd.DataFrame()\n",
    "for column, model in best_models.items():\n",
    "    Y_pred_KRR[column] = model.predict(X_test_transformed[:k, :])\n",
    "\n",
    "# Afficher les prédictions\n",
    "#print(Y_pred_KRR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1e526",
   "metadata": {},
   "source": [
    "## Gradient boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6aac8-fd7c-4dd7-bca3-57cf0f6969c5",
   "metadata": {},
   "source": [
    "### Définition\n",
    "Le **Gradient Boosting** est une technique d'ensemble qui construit un modèle puissant en combinant plusieurs modèles faibles (souvent des arbres de décision) de manière séquentielle. À chaque étape, le modèle suivant corrige les erreurs du modèle précédent en optimisant une fonction de perte grâce à la descente de gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "L'objectif est de minimiser une fonction de perte $L(Y, \\hat{Y})$, où :\n",
    "- $Y$ : Cibles réelles.\n",
    "- $\\hat{Y}$ : Prédictions du modèle.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparamètres clés\n",
    "- **Nombre d'estimateurs** $(n\\_estimators)$ : Nombre total d'arbres.\n",
    "- **Taux d'apprentissage** $(learning\\_rate)$ : Contrôle la contribution de chaque arbre.\n",
    "- **Profondeur maximale** $(max\\_depth)$ : Limite la complexité des arbres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d02f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MEC_530' enregistré sous Best_models/X_Y/GB\\MEC_530_best_model_GBR.joblib\n",
      "Variable de sortie 'MEC_530'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -0.006517991665508269\n",
      "MSE: 2.9269037122651693e+22, MAPE: 0.018920026578133128, R²: 0.9835475504042313\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MEC_467' enregistré sous Best_models/X_Y/GB\\MEC_467_best_model_GBR.joblib\n",
      "Variable de sortie 'MEC_467'\n",
      "Meilleurs paramètres : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -0.017701290659196477\n",
      "MSE: 1.8608611628679557e+23, MAPE: 0.03248445439501293, R²: 0.8947333898910467\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_530' enregistré sous Best_models/X_Y/GB\\Cbac_530_best_model_GBR.joblib\n",
      "Variable de sortie 'Cbac_530'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -2.3053715787009024\n",
      "MSE: 3.234943699304142e-07, MAPE: 14.551132647064811, R²: 0.8752053490839681\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_467' enregistré sous Best_models/X_Y/GB\\Cbac_467_best_model_GBR.joblib\n",
      "Variable de sortie 'Cbac_467'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 100}\n",
      "Meilleur score (MAPE négatif) : -10.773845441638047\n",
      "MSE: 2.708505473463389e-06, MAPE: 144.82403421317485, R²: 0.5504738509128528\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_530' enregistré sous Best_models/X_Y/GB\\MBC_530_best_model_GBR.joblib\n",
      "Variable de sortie 'MBC_530'\n",
      "Meilleurs paramètres : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -0.0357095787566834\n",
      "MSE: 2.244161578131232e+19, MAPE: 0.05437008002287685, R²: 0.9874870322932354\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_467' enregistré sous Best_models/X_Y/GB\\MBC_467_best_model_GBR.joblib\n",
      "Variable de sortie 'MBC_467'\n",
      "Meilleurs paramètres : {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -0.07238402770162952\n",
      "MSE: 1.6159789793102283e+20, MAPE: 0.10233276685822032, R²: 0.956869173118657\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_530' enregistré sous Best_models/X_Y/GB\\LR_530_best_model_GBR.joblib\n",
      "Variable de sortie 'LR_530'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -0.0185308546561719\n",
      "MSE: 30547.18729563514, MAPE: 0.16080719569191448, R²: 0.7471062155428679\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_467' enregistré sous Best_models/X_Y/GB\\LR_467_best_model_GBR.joblib\n",
      "Variable de sortie 'LR_467'\n",
      "Meilleurs paramètres : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 500}\n",
      "Meilleur score (MAPE négatif) : -0.04078909152762396\n",
      "MSE: 11369.185260370821, MAPE: 0.20859265931863286, R²: 0.8249123734437733\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour GradientBoostingRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 300, 500],\n",
    "    'learning_rate': [0.05, 0.1, 0.5],\n",
    "    'max_depth': [2, 3]\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# DataFrame pour stocker les prédictions pour chaque variable cible\n",
    "Y_pred_GB = pd.DataFrame()\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/X_Y/GB'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable de sortie (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle de GradientBoostingRegressor\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    \n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=gbr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "    \n",
    "    # Enregistrer le meilleur modèle pour la variable cible actuelle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "    \n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_pred = best_model.predict(X_test_transformed)\n",
    "    y_true = Y_test.iloc[:, i]\n",
    "    \n",
    "    # Ajouter les prédictions au DataFrame Y_pred_GB\n",
    "    Y_pred_GB[col] = y_pred\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Ajouter les meilleurs paramètres et les scores dans la liste des meilleurs paramètres\n",
    "    best_params_list.append({\n",
    "        'Variable': col,\n",
    "        'n_estimators': grid_search.best_params_['n_estimators'],\n",
    "        'learning_rate': grid_search.best_params_['learning_rate'],\n",
    "        'max_depth': grid_search.best_params_['max_depth'],\n",
    "        'Best Score (MAPE Negatif)': grid_search.best_score_,\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_GBR.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les meilleurs hyperparamètres et les scores pour la variable cible actuelle\n",
    "    print(f\"Variable de sortie '{col}'\")\n",
    "    print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "    print(\"Meilleur score (MAPE négatif) :\", grid_search.best_score_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour stocker les meilleurs paramètres et les métriques de chaque modèle\n",
    "params_GB = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le DataFrame des meilleurs paramètres\n",
    "#print(params_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f5c8b",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805c135-4919-4397-80df-d6ada05eb0a4",
   "metadata": {},
   "source": [
    "### Définition\n",
    "XGBoost est une implémentation avancée et optimisée de la méthode Gradient Boosting. Elle est conçue pour être :\n",
    "- **Rapide** grâce à des optimisations matérielles et algorithmiques.\n",
    "- **Précise** avec des techniques intégrées de régularisation.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "\n",
    "L'objectif est de minimiser une fonction de perte régulière définie par : \n",
    "\n",
    "$ \\mathcal{L}(\\Theta) = \\sum_{i=1}^{n} L(Y_i, \\hat{Y}_i) + \\sum_{k=1}^{K} \\Omega(f_k) $\n",
    "- $L(Y_i, \\hat{Y}_i)$ : Fonction de perte\n",
    "- $\\Omega(f_k)$ : Terme de régularisation pour éviter le surapprentissage.\n",
    "\n",
    "$ \\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2 $\n",
    "  - $T$ : Nombre de feuilles dans l'arbre.\n",
    "  - $w$ : Poids des feuilles.\n",
    "  - $\\gamma, \\lambda$ : Hyperparamètres de régularisation.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparamètres clés\n",
    "- **Nombre d'estimateurs** $(n\\_estimators)$ : Nombre total d'arbres.\n",
    "- **Taux d'apprentissage** $(learning\\_rate)$ : Contrôle la contribution de chaque arbre.\n",
    "- **Profondeur maximale** $(max\\_depth)$ : Limite la complexité des arbres.\n",
    "- **Subsample** et **ColSampleByTree** : Contrôle du sous-échantillonnage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c210111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MEC_530' enregistré sous Best_models/X_Y/XGB\\MEC_530_best_model_XGB.joblib\n",
      "Target Column 'MEC_530'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 1.8791987031530614e+23, MAPE: 0.045250678450007485, R²: 0.8943681617727961\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MEC_467' enregistré sous Best_models/X_Y/XGB\\MEC_467_best_model_XGB.joblib\n",
      "Target Column 'MEC_467'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 6.25693583348061e+23, MAPE: 0.06757855980851712, R²: 0.646052893142959\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_530' enregistré sous Best_models/X_Y/XGB\\Cbac_530_best_model_XGB.joblib\n",
      "Target Column 'Cbac_530'\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "MSE: 9.548404577227866e-07, MAPE: 25.920908481016554, R²: 0.6316505241570318\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_467' enregistré sous Best_models/X_Y/XGB\\Cbac_467_best_model_XGB.joblib\n",
      "Target Column 'Cbac_467'\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "MSE: 3.080342116660399e-06, MAPE: 19.748496444266046, R²: 0.4887607416193698\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_530' enregistré sous Best_models/X_Y/XGB\\MBC_530_best_model_XGB.joblib\n",
      "Target Column 'MBC_530'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 9.79741434820926e+20, MAPE: 0.29056284237588487, R²: 0.45371701153968846\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'MBC_467' enregistré sous Best_models/X_Y/XGB\\MBC_467_best_model_XGB.joblib\n",
      "Target Column 'MBC_467'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 2.131436242623227e+21, MAPE: 0.3172208845393677, R²: 0.43111507781838454\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_530' enregistré sous Best_models/X_Y/XGB\\LR_530_best_model_XGB.joblib\n",
      "Target Column 'LR_530'\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 21953.89146976524, MAPE: 0.2680928242054923, R²: 0.8182483171488787\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_467' enregistré sous Best_models/X_Y/XGB\\LR_467_best_model_XGB.joblib\n",
      "Target Column 'LR_467'\n",
      "Best Parameters: {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500, 'subsample': 0.8}\n",
      "MSE: 8927.163690851, MAPE: 0.3043035050449401, R²: 0.8625199724769859\n",
      "----------------------------------------\n",
      "  Target Column  n_estimators  learning_rate  max_depth  subsample  \\\n",
      "0       MEC_530           500           0.10          7        0.8   \n",
      "1       MEC_467           500           0.05          7        0.8   \n",
      "2      Cbac_530           100           0.30          3        0.8   \n",
      "3      Cbac_467           100           0.30          5        0.8   \n",
      "4       MBC_530           500           0.10          7        0.8   \n",
      "5       MBC_467           500           0.05          7        0.8   \n",
      "6        LR_530           500           0.10          7        0.8   \n",
      "7        LR_467           500           0.10          5        0.8   \n",
      "\n",
      "   colsample_bytree           MSE       MAPE        R2  \n",
      "0               1.0  1.879199e+23   0.045251  0.894368  \n",
      "1               1.0  6.256936e+23   0.067579  0.646053  \n",
      "2               0.8  9.548405e-07  25.920908  0.631651  \n",
      "3               0.8  3.080342e-06  19.748496  0.488761  \n",
      "4               1.0  9.797414e+20   0.290563  0.453717  \n",
      "5               1.0  2.131436e+21   0.317221  0.431115  \n",
      "6               0.8  2.195389e+04   0.268093  0.818248  \n",
      "7               1.0  8.927164e+03   0.304304  0.862520  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Nombre d'arbres\n",
    "    'learning_rate': [0.05, 0.1, 0.3],  # Taux d'apprentissage\n",
    "    'max_depth': [3, 5, 7],  # Profondeur maximale des arbres\n",
    "    'subsample': [0.8, 1],  # Fraction des échantillons utilisés pour entraîner chaque arbre\n",
    "    'colsample_bytree': [0.8, 1]  # Fraction des caractéristiques utilisées pour chaque arbre\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/X_Y/XGB'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle XGBoost\n",
    "    xgbr = XGBRegressor(objective='reg:squarederror', n_jobs=-1)  # Configuré pour minimiser l'erreur quadratique\n",
    "\n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgbr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',  # Optimisation avec MAPE\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "\n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "\n",
    "    # Prédire sur les données de test\n",
    "    y_pred = best_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "\n",
    "    # Calculer les erreurs\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Stocker les résultats\n",
    "    best_params_list.append({\n",
    "        'Target Column': col,\n",
    "        'n_estimators': grid_search.best_params_['n_estimators'],\n",
    "        'learning_rate': grid_search.best_params_['learning_rate'],\n",
    "        'max_depth': grid_search.best_params_['max_depth'],\n",
    "        'subsample': grid_search.best_params_['subsample'],\n",
    "        'colsample_bytree': grid_search.best_params_['colsample_bytree'],\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_XGB.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les résultats pour chaque colonne\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_XGB = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_XGB)\n",
    "\n",
    "# Créer un DataFrame pour les prédictions\n",
    "Y_pred_XGB = pd.DataFrame()\n",
    "for column, model in best_models.items():\n",
    "    Y_pred_XGB[column] = model.predict(X_test_transformed[:k, :])\n",
    "\n",
    "# Afficher les prédictions (si nécessaire)\n",
    "# print(Y_pred_XGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f3dcb",
   "metadata": {},
   "source": [
    "## ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d865c01-d8c0-46c7-983b-dc6f353bd221",
   "metadata": {},
   "source": [
    "### Définition\n",
    "Les **réseaux de neurones artificiels** (ANN) sont des modèles inspirés du cerveau humain, capables de détecter des motifs complexes dans les données. Ils sont constitués de couches de **neurones** interconnectés, organisées en trois types principaux de couches :\n",
    "- **Entrée** : Reçoit les données d'entrée.\n",
    "- **Cachées** : Effectuent les transformations et calculs complexes.\n",
    "- **Sortie** : Produit les prédictions finales.\n",
    "\n",
    "---\n",
    "\n",
    "### Modèle mathématique\n",
    "\n",
    "Chaque neurone dans une couche effectue un calcul basé sur une somme pondérée des entrées, suivie d'une activation non linéaire :\n",
    "\n",
    "$ z = \\sum_{i=1}^{n} w_i x_i + b $\n",
    "\n",
    "$a = \\phi(z)$\n",
    "- $w_i$ : Poids associés aux entrées.\n",
    "- $x_i$ : Entrées.\n",
    "- $b$ : Biais.\n",
    "- $\\phi$ : Fonction d'activation (ex. ReLU, sigmoïde, tanh).\n",
    "\n",
    "Les poids et les biais sont ajustés pendant l'entraînement pour minimiser une fonction de perte.\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparamètres clés\n",
    "\n",
    "- **hidden_layer_sizes** : Détermine la structure des couches cachées. Chaque tuple dans cette liste représente le nombre de neurones dans chaque couche cachée.\n",
    "- **activation** : Fonction d'activation à utiliser dans les couches cachées.\n",
    "- **learning_rate_init** : Taux d'apprentissage initial, contrôlant la vitesse à laquelle le modèle ajuste ses poids.\n",
    "- **max_iter** : Nombre maximal d'itérations (ou d'époques) pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974cf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MEC_530' enregistré sous Best_models/X_Y/ANN\\MEC_530_best_model_ANN.joblib\n",
      "Target Column 'MEC_530'\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.01, 'max_iter': 1000}\n",
      "MSE: 2.9952738711980647e+25, MAPE: 0.9754552332509648, R²: -15.836765823523152\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MEC_467' enregistré sous Best_models/X_Y/ANN\\MEC_467_best_model_ANN.joblib\n",
      "Target Column 'MEC_467'\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.01, 'max_iter': 1000}\n",
      "MSE: 5.3853549590869e+25, MAPE: 0.9815611401371303, R²: -29.46428568065855\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_530' enregistré sous Best_models/X_Y/ANN\\Cbac_530_best_model_ANN.joblib\n",
      "Target Column 'Cbac_530'\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.01, 'max_iter': 500}\n",
      "MSE: 1.7473512077433856e-05, MAPE: 2814.0240686529173, R²: -5.740769060214211\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'Cbac_467' enregistré sous Best_models/X_Y/ANN\\Cbac_467_best_model_ANN.joblib\n",
      "Target Column 'Cbac_467'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.01, 'max_iter': 500}\n",
      "MSE: 2.742227913237101e-05, MAPE: 1588.72602819202, R²: -3.551230063347406\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MBC_530' enregistré sous Best_models/X_Y/ANN\\MBC_530_best_model_ANN.joblib\n",
      "Target Column 'MBC_530'\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01, 'max_iter': 1000}\n",
      "MSE: 3.3594208006152116e+21, MAPE: 0.766356299701995, R²: -0.8731415955590784\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'MBC_467' enregistré sous Best_models/X_Y/ANN\\MBC_467_best_model_ANN.joblib\n",
      "Target Column 'MBC_467'\n",
      "Best Parameters: {'activation': 'relu', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.01, 'max_iter': 1000}\n",
      "MSE: 5.123182497637579e+21, MAPE: 0.8589016233279533, R²: -0.3673884389353341\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle pour la variable 'LR_530' enregistré sous Best_models/X_Y/ANN\\LR_530_best_model_ANN.joblib\n",
      "Target Column 'LR_530'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 100), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
      "MSE: 20375.282312946118, MAPE: 0.11345095486637055, R²: 0.8313172926975281\n",
      "----------------------------------------\n",
      "Modèle pour la variable 'LR_467' enregistré sous Best_models/X_Y/ANN\\LR_467_best_model_ANN.joblib\n",
      "Target Column 'LR_467'\n",
      "Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.001, 'max_iter': 1000}\n",
      "MSE: 22902.852592739047, MAPE: 0.10582892344359723, R²: 0.6472916915333112\n",
      "----------------------------------------\n",
      "  Target Column hidden_layer_sizes activation  learning_rate_init  max_iter  \\\n",
      "0       MEC_530         (100, 100)       relu               0.010      1000   \n",
      "1       MEC_467         (100, 100)       relu               0.010      1000   \n",
      "2      Cbac_530         (100, 100)       relu               0.010       500   \n",
      "3      Cbac_467         (100, 100)       tanh               0.010       500   \n",
      "4       MBC_530           (50, 50)       relu               0.010      1000   \n",
      "5       MBC_467         (100, 100)       relu               0.010      1000   \n",
      "6        LR_530         (100, 100)       tanh               0.001      1000   \n",
      "7        LR_467           (50, 50)       tanh               0.001      1000   \n",
      "\n",
      "            MSE         MAPE         R2  \n",
      "0  2.995274e+25     0.975455 -15.836766  \n",
      "1  5.385355e+25     0.981561 -29.464286  \n",
      "2  1.747351e-05  2814.024069  -5.740769  \n",
      "3  2.742228e-05  1588.726028  -3.551230  \n",
      "4  3.359421e+21     0.766356  -0.873142  \n",
      "5  5.123182e+21     0.858902  -0.367388  \n",
      "6  2.037528e+04     0.113451   0.831317  \n",
      "7  2.290285e+04     0.105829   0.647292  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misse\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from joblib import dump  # Importation de joblib pour sauvegarder les modèles\n",
    "\n",
    "# Paramètres pour GridSearchCV pour MLPRegressor (ANN)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Architectures des couches cachées\n",
    "    'activation': ['relu', 'tanh'],                               # Fonctions d'activation\n",
    "    'learning_rate_init': [0.001, 0.01],                          # Taux d'apprentissage initial\n",
    "    'max_iter': [500, 1000]                                       # Nombre maximal d'itérations\n",
    "}\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les meilleurs modèles pour chaque variable cible\n",
    "best_models = {}\n",
    "best_params_list = []\n",
    "\n",
    "# Créer le dossier pour les modèles si il n'existe pas\n",
    "output_dir = 'Best_models/X_Y/ANN'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Boucle sur chaque variable cible (chaque colonne de Y_train)\n",
    "for i, col in enumerate(Y_train.columns):\n",
    "    # Initialiser un modèle ANN (MLPRegressor)\n",
    "    mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "    # Configurer la recherche de grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=mlp,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',  # Optimisation avec MAPE\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Ajuster le modèle sur la colonne actuelle de Y_train\n",
    "    grid_search.fit(X_train_transformed[:k, :], Y_train.iloc[:k, i])\n",
    "\n",
    "    # Obtenir le meilleur modèle\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[col] = best_model\n",
    "\n",
    "    # Prédire sur les données de test\n",
    "    y_pred = best_model.predict(X_test_transformed[:k, :])\n",
    "    y_true = Y_test.iloc[:k, i]\n",
    "\n",
    "    # Calculer les erreurs\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Stocker les résultats\n",
    "    best_params_list.append({\n",
    "        'Target Column': col,\n",
    "        'hidden_layer_sizes': grid_search.best_params_['hidden_layer_sizes'],\n",
    "        'activation': grid_search.best_params_['activation'],\n",
    "        'learning_rate_init': grid_search.best_params_['learning_rate_init'],\n",
    "        'max_iter': grid_search.best_params_['max_iter'],\n",
    "        'MSE': mse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "    # Sauvegarder le modèle pour chaque colonne dans le dossier spécifié\n",
    "    model_path = os.path.join(output_dir, f'{col}_best_model_ANN.joblib')  # Sauvegarder dans le bon dossier\n",
    "    dump(best_model, model_path)\n",
    "    print(f\"Modèle pour la variable '{col}' enregistré sous {model_path}\")\n",
    "\n",
    "    # Afficher les résultats pour chaque colonne\n",
    "    print(f\"Target Column '{col}'\")\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(f\"MSE: {mse}, MAPE: {mape}, R²: {r2}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Créer un DataFrame pour résumer les résultats\n",
    "params_ANN = pd.DataFrame(best_params_list)\n",
    "\n",
    "# Afficher le tableau des résultats\n",
    "print(params_ANN)\n",
    "\n",
    "# Créer un DataFrame pour les prédictions\n",
    "Y_pred_ANN = pd.DataFrame()\n",
    "for column, model in best_models.items():\n",
    "    Y_pred_ANN[column] = model.predict(X_test_transformed[:k, :])\n",
    "\n",
    "# Afficher les prédictions (si nécessaire)\n",
    "# print(Y_pred_ANN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
